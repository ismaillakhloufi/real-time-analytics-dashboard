# -*- coding: utf-8 -*-
"""Spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WMK_aR9f0Oc2P-tHaNg8Uml7PbtsaYBl
"""

# prompt: developper bien ce code svp (from pyspark.sql import SparkSession
# from pyspark.sql.functions import col, avg, max, min
# # Créer une session Spark
# spark = SparkSession.builder \
#     .appName("Real-Time Analytics Dashboard - Spark") \
#     .getOrCreate()
# # Charger les données CSV
# data_path = "/content/indexProcessed.csv"  # Assurez-vous que le fichier CSV est dans ce chemin
# df = spark.read.csv(data_path, header=True, inferSchema=True)
# # Nettoyer les données (supprimer les lignes nulles)
# df_cleaned = df.na.drop()
# # Ajouter une colonne Year pour regrouper par année
# df_cleaned = df_cleaned.withColumn("Year", col("Date").substr(1, 4))
# # Calculs statistiques
# df_stats = df_cleaned.groupBy("Year").agg(
#     avg(col("Close")).alias("Avg_Close"),
#     max(col("Close")).alias("Max_Close"),
#     min(col("Close")).alias("Min_Close")
# )
# # Afficher les résultats
# df_stats.show()
# # Enregistrer les résultats dans un fichier CSV
# output_path = "output/stats_by_year"
# df_stats.write.csv(output_path, header=

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, max, min

# Créer une session Spark
spark = SparkSession.builder \
    .appName("Real-Time Analytics Dashboard - Spark") \
    .getOrCreate()

# Charger les données CSV
data_path = "/content/indexProcessed.csv"  # Assurez-vous que le fichier CSV est dans ce chemin
df = spark.read.csv(data_path, header=True, inferSchema=True)

# Nettoyer les données (supprimer les lignes nulles)
df_cleaned = df.na.drop()

# Extraire l'année de la colonne Date
df_cleaned = df_cleaned.withColumn("Year", col("Date").substr(1, 4))

# Calculs statistiques par année
df_stats = df_cleaned.groupBy("Year") \
    .agg(
        avg("Close").alias("Avg_Close"),
        max("Close").alias("Max_Close"),
        min("Close").alias("Min_Close")
    )

# Afficher les résultats
df_stats.show()

# Enregistrer les résultats dans un fichier CSV (remplacez "output/stats_by_year" par le chemin souhaité)
output_path = "/content/output.csv"
df_stats.write.csv(output_path, header=True, mode="overwrite")

# Arrêter la session Spark
spark.stop()

